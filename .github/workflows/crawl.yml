name: Job Crawler and Email Notification

on:
  schedule:
    - cron: '30 15 * * *'  # 北京时间23:30
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.10'
  EMAIL_USER: ${{ secrets.EMAIL_USER }}
  EMAIL_PWD: ${{ secrets.EMAIL_PWD }}

jobs:
  crawl-and-notify:
    runs-on: ubuntu-latest
    
    steps:
    # 1. 检出代码（获取完整提交历史）
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # 关键：获取完整提交历史
    
    # 2. 设置Python环境
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    # 3. 安装依赖
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        sudo apt-get update
        sudo apt-get install -y chromium-browser chromium-chromedriver
    
    # 4. 运行爬虫（脚本名称可能需要调整）
    - name: Run crawler
      run: python crawler_github.py  # 确保脚本名称正确
    
    # 5. 自动提交JSON文件到仓库（永久保存）- 修改为提交两个JSON文件
    - name: Commit and push JSON data
      if: success()  # 只有爬虫成功才提交
      run: |
        # 配置Git用户
        git config --global user.name "GitHub Actions"
        git config --global user.email "actions@github.com"
        
        # 检查是否有变更（避免空提交）
        git add campus_jobs.json intern_jobs.json  # 添加两个文件
        git status
        
        if git diff-index --quiet HEAD; then
          echo "没有数据变更，跳过提交"
        else
          git commit -m "Auto-update job data (campus and intern) [skip ci]"
          git push
        fi
    
    # 6. 上传Excel作为临时制品（修改为上传两个Excel文件）
    - name: Upload Excel artifacts
      uses: actions/upload-artifact@v4
      with:
        name: job_data_excel
        path: |
          campus_jobs.xlsx
          intern_jobs.xlsx
        retention-days: 1  # 只保留1天
